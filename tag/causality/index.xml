<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>causality | Welcome!</title>
    <link>https://jmenter-93.github.io/tag/causality/</link>
      <atom:link href="https://jmenter-93.github.io/tag/causality/index.xml" rel="self" type="application/rss+xml" />
    <description>causality</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 01 Apr 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://jmenter-93.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>causality</title>
      <link>https://jmenter-93.github.io/tag/causality/</link>
    </image>
    
    <item>
      <title>The Validity of Counterfactual Reasoning for Fair Prediction with Imperfect Models</title>
      <link>https://jmenter-93.github.io/project/ms-counterfactuals/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://jmenter-93.github.io/project/ms-counterfactuals/</guid>
      <description>&lt;p&gt;Advisor:  Dr. David Jensen (University of Massachusetts, Amherst)&lt;/p&gt;
&lt;p&gt;Existing work on machine learning for fair prediction has leveraged causal reasoning to account for the counterfactual nature of societal discrimination. Methods that leverage this reasoning require information about the true data-generating process (DGP). However, the true DGP is often subject to mechanisms of model misspecification, including latent confounding, measurement error, and selection bias, all potentially resulting from societal discrimination. When is the validity of counterfactual reasoning for fairness impacted by these mechanisms? To articulate an answer, we describe distributional parities that must hold in training data in order for Y hat to be counterfactually fair, regardless of the algorithm or estimator used to train Y hat. Using the syntax of graphical models and d-separation, we describe the structures of model misspecification that (do not) threaten these requirements. We note that well-documented societal biases in observed data can result in model misspecifications that cause Y hat to violate counterfactual fairness. Empirically, we use a real dataset to investigate the impact of these forms of model misspecification on the task of fair prediction.&lt;/p&gt;
&lt;p&gt;Read our white paper &lt;a href=&#34;./Menter_cf_validity_v1.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
